{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model for sentiment classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chakrapani/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import GPT2TokenizerFast\n",
    "import concurrent.futures\n",
    "from helper import *\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "console_logger = logging.getLogger(__name__)\n",
    "console_logger.setLevel(Config.LOG_LEVEL)\n",
    "console_logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read csv to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T_0</td>\n",
       "      <td>I have to confess that I am severely disappoin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T_9</td>\n",
       "      <td>I have never understood the appeal of this sho...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T_12</td>\n",
       "      <td>This is supposed to be based on Wilkie Collins...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T_13</td>\n",
       "      <td>Of all the British imperialist movies like Fou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T_15</td>\n",
       "      <td>I loved this film. Not being a swooning Ed Woo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                             review sentiment\n",
       "0       T_0  I have to confess that I am severely disappoin...  negative\n",
       "1       T_9  I have never understood the appeal of this sho...  negative\n",
       "2      T_12  This is supposed to be based on Wilkie Collins...  negative\n",
       "3      T_13  Of all the British imperialist movies like Fou...  positive\n",
       "4      T_15  I loved this film. Not being a swooning Ed Woo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_path = \"sentiments_train_test_reviews/reviews_test_4000.csv\"\n",
    "df = pd.read_csv(datafile_path)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check whether all reviews fall within token_limmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  review_id                                             review sentiment  \\\n",
      "0       T_0  I have to confess that I am severely disappoin...  negative   \n",
      "1       T_9  I have never understood the appeal of this sho...  negative   \n",
      "2      T_12  This is supposed to be based on Wilkie Collins...  negative   \n",
      "3      T_13  Of all the British imperialist movies like Fou...  positive   \n",
      "4      T_15  I loved this film. Not being a swooning Ed Woo...  positive   \n",
      "\n",
      "   n_tokens  \n",
      "0       320  \n",
      "1       177  \n",
      "2        84  \n",
      "3       264  \n",
      "4       617  \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "encoding_name = \"cl100k_base\"\n",
    "#usage for : num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
    "\n",
    "#df['n_tokens'] = df.review.apply(lambda x: len(tokenizer.encode(x)))\n",
    "df['n_tokens'] = df.review.apply(lambda x: num_tokens_from_string(x,encoding_name))\n",
    "print(df.head())\n",
    "print((df['n_tokens'] > 8000).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1173420"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((df['n_tokens'] > 8000).sum())\n",
    "total_count = df['n_tokens'].sum()\n",
    "total_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set the model used for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GPT3\"\n",
    "engine, MAX_TOKENS,dimensions = embed_gen_model(model_name)\n",
    "first_time = True  #True if running the notebook for the first time\n",
    "if first_time:\n",
    "    (df['n_tokens'] > MAX_TOKENS).sum()\n",
    "    df.to_csv('partial_embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002 8191 1536\n"
     ]
    }
   ],
   "source": [
    "print(engine,MAX_TOKENS,dimensions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN the below cell only for generating embeddings - not always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total embeddings to be generated = 4000\n",
      " 25%|██▌       | 1000/4000 [31:37<1:02:34,  1.25s/it]1000 embeddings generated and saved to partial_embeddings.csv.\n",
      " 50%|█████     | 2000/4000 [1:05:17<1:22:33,  2.48s/it]2000 embeddings generated and saved to partial_embeddings.csv.\n",
      " 75%|███████▌  | 3000/4000 [1:40:01<47:38,  2.86s/it]  3000 embeddings generated and saved to partial_embeddings.csv.\n",
      "100%|██████████| 4000/4000 [2:14:05<00:00,  2.31s/it]  4000 embeddings generated and saved to partial_embeddings.csv.\n",
      "100%|██████████| 4000/4000 [2:14:13<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "save_csv_path = 'test_reviews_with_embeddings.csv'\n",
    "#tqdm.pandas()\n",
    "if model_name != \"GPT3\":    \n",
    "    # use a ThreadPoolExecutor to get the embeddings in parallel - 30secs for 1000 embeddings\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # pass the engine parameter to the get_embeddings function\n",
    "        results = [executor.submit(get_embeddings, text, engine) for text in df.review]\n",
    "        df['embeddings'] = [r.result() for r in results]\n",
    "else:\n",
    "    #df['embeddings'] = df.review.progress_apply(lambda x: embeddings(x, engine))\n",
    "    df = pd.read_csv('partial_embeddings.csv')\n",
    "    if 'embeddings' not in df.columns:\n",
    "        df['embeddings'] = pd.Series(dtype=object)\n",
    "    no_of_rows = df['embeddings'].isna().sum()\n",
    "    console_logger.info(f\"Total embeddings to be generated = {no_of_rows}\")\n",
    "    #print(f\"Total embeddings to be generated = {no_of_rows}\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        with tqdm(total=no_of_rows) as pbar:\n",
    "            future_to_index = {executor.submit(embeddings, row['review'],engine): i for i, row in df.iterrows() if pd.isnull(row['embeddings'])}\n",
    "            for future in concurrent.futures.as_completed(future_to_index):\n",
    "                i = future_to_index[future]\n",
    "                try:\n",
    "                    embedding = future.result()\n",
    "                    console_logger.debug(f\"embeddings of {i} = {embedding}\")\n",
    "                    console_logger.debug(f\"shape of df = {df.shape}\")#, shape of embeddings = {embeddings.shape}\")\n",
    "                    df.at[i, \"embeddings\"] = embedding\n",
    "                    pbar.update()\n",
    "                    if (i+1) % 1000 == 0:\n",
    "                        df.to_csv(f\"partial_embeddings.csv\", index=False)\n",
    "                        #print(f\"{i+1} embeddings generated and saved to partial_embeddings.csv.\")\n",
    "                        console_logger.info(f\"{i+1} embeddings generated and saved to partial_embeddings.csv.\")\n",
    "                except Exception as exc:\n",
    "                    df.to_csv(f\"partial_embeddings.csv\", index=False)\n",
    "                    console_logger.error(f\"An error occurred: {exc}. Saving partial embeddings to partial_embeddings.csv and exiting.\")\n",
    "df.to_csv(save_csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6424b97f06403314f4be31d32811ce911cda5159b72245e4be57bd5745a3201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
